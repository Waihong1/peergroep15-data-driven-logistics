{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voeg je code toe in de behorende codeblok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "df = pd.read_csv('trainingsset.csv')\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hashtags naar None\n",
    "df.replace('#', None, inplace=True)\n",
    "\n",
    "# onrealistische waarden naar None\n",
    "df.loc[df['reistijd_a'] < 28, 'reistijd_a'] = None\n",
    "df.loc[df['reistijd_c'] == 0, 'reistijd_c'] = None\n",
    "df.loc[df['reistijd_a'] == None, 'reistijd_totaal'] = None\n",
    "df.loc[df['reistijd_c'] == None, 'reistijd_totaal'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "#### Tijd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering m.b.t. starttijd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering m.b.t. windrichting, windkracht en windsnelheid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random feature engineering die ik random heb uitgevoerd weet niet of het handig gaat zijn maargoed\n",
    "dit gaat nog worden opgeschoond en verdeeld onder andere functies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in geval niet gedaan bij tijd\n",
    "\n",
    "def plink(df):\n",
    "    df['start_hour'] = pd.to_datetime(df['datumtijd_startpunt']).dt.hour\n",
    "    df['start_day_of_week'] = pd.to_datetime(df['datumtijd_startpunt']).dt.dayofweek\n",
    "    df['start_month'] = pd.to_datetime(df['datumtijd_startpunt']).dt.month\n",
    "    df['start_week_of_year'] = pd.to_datetime(df['datumtijd_startpunt']).dt.isocalendar().week\n",
    "    df['is_weekend'] = df['start_day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "    # in geval niet gedaan bij wind\n",
    "    df['wind_speed_ship_length_interaction'] = df['windsnelheid'] * df['lengte_schip_huidig']\n",
    "    df['wind_direction_trip_distance_interaction'] = df['windrichting'] * df['vaarafstand_totaal_in_km']\n",
    "    df['total_wind_impact'] = df.groupby(['windrichting'])['windsnelheid'].transform('sum')\n",
    "    df['mean_wind_speed_per_harbor'] = df.groupby('havenbekken')['windsnelheid'].transform('mean')\n",
    "\n",
    "    wind_bins = [0, 8, 16, 20, 24] \n",
    "    wind_labels = ['calm', 'windy', 'windy-stormy', 'stormy']\n",
    "    df['wind_speed_category'] = pd.cut(df['windsnelheid'], bins=wind_bins, labels=wind_labels)\n",
    "\n",
    "    df['wind_speed_direction_interaction'] = df['windsnelheid'] * df['windrichting']\n",
    "\n",
    "    # logging\n",
    "    df['log_ship_length'] = np.log1p(df['lengte_schip_huidig'])\n",
    "    df['log_trip_distance'] = np.log1p(df['vaarafstand_totaal_in_km'])\n",
    "\n",
    "    # normalize\n",
    "    for col in ['vaarafstand_a_in_km', 'vaarafstand_b_in_km', 'vaarafstand_c_in_km', 'vaarafstand_totaal_in_km']:\n",
    "        df[col + '_normalized'] = (df[col] - df[col].mean()) / df[col].std()\n",
    "\n",
    "    # bekken \n",
    "    berth_usage = df['berth_name'].value_counts().to_dict()\n",
    "    df['berth_utilization'] = df['berth_name'].map(berth_usage)\n",
    "\n",
    "    havenbekken_usage = df['havenbekken'].value_counts().to_dict()\n",
    "    df['bekken_utilization'] = df['havenbekken'].map(havenbekken_usage)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(df, is_train=True):\n",
    "    # first define which columns to scale (do not scale one-hot encoded columns and target columns)\n",
    "    numerical_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "    non_numerical_cols = df.select_dtypes(exclude=['number']).columns.tolist()\n",
    "    exclusions = ['id', 'reistijd_a', 'reistijd_b', 'reistijd_c', 'reistijd_totaal'] # exclude targets and one-hot encoded data \n",
    "\n",
    "    boolean_like_cols = [col for col in numerical_cols if df[col].dropna().isin([0, 1]).all()]\n",
    "    \n",
    "    cols_to_scale = [col for col in numerical_cols if col not in exclusions and col not in boolean_like_cols] \n",
    "    cols_not_to_scale = non_numerical_cols + exclusions + boolean_like_cols\n",
    "    \n",
    "    # if it's training data, fit the scaler and save it. If it's test data, load the scaler\n",
    "    if is_train:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(df[cols_to_scale])\n",
    "        \n",
    "        with open('scaler.pkl', 'wb') as file:\n",
    "            pickle.dump(scaler, file)\n",
    "    else: \n",
    "        with open('scaler.pkl', 'rb') as file:\n",
    "            scaler = pickle.load(file)\n",
    "         \n",
    "    scaled_data = scaler.transform(df[cols_to_scale])\n",
    "    scaled_df = pd.DataFrame(scaled_data, columns=cols_to_scale)\n",
    "    scaled_df = pd.concat([scaled_df, df[cols_not_to_scale].reset_index()], axis=1)\n",
    "    \n",
    "    return scaled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category_limit meaning the number of unique values in a column \n",
    "def onehot_encode(df, columns_to_encode=None):\n",
    "    # transform objects to category\n",
    "    object_columns = df.select_dtypes(include='object').columns\n",
    "    df[object_columns] = df[object_columns].astype('category')\n",
    "    \n",
    "    # Get columns to one-hot encode if it exists given by a parameter in the function\n",
    "    columns_to_encode = [col for col in columns_to_encode if col in df.columns]\n",
    "    \n",
    "    # One-hot encode the appropriate columns\n",
    "    df = pd.get_dummies(df, columns=columns_to_encode, drop_first=False, dtype=int)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Hier komt een dictionary met een key,value pair van de column, mappings -> mapping is een dict van categorical -> ordinal values\n",
    "def ordinal_encode(df, encoding_dict=None):\n",
    "    # transform objects to category\n",
    "    object_columns = df.select_dtypes(include='object').columns\n",
    "    df[object_columns] = df[object_columns].astype('category')\n",
    "    \n",
    "    #Get columns to ordinal  encode if it exists given by a parameter in the function\n",
    "    for k, v in encoding_dict.items():\n",
    "        df[k] = df[k].map(v)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "ga ik maken zodra feature engineering van wind en tijd is afgerond"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
